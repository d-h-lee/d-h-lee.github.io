---
title: Bypassing Stationary Points in Training Deep Learning Models
authors:
- Jaeheun Jung
- admin
date: '2024-06-25'
publishDate: '2024-06-25T01:23:45.678901Z'
publication_types:
- article-journal
publication: '*IEEE Transactions on Neural Networks and Learning Systems*'
publication_short: In *IEEE TNNLS*

abstract: 'Gradient-descent-based optimizers are prone to slowdowns in training deep learning models, as stationary points are ubiquitous in the loss landscape of most neural networks. We present an intuitive concept of bypassing the stationary points and realize the concept into a novel method designed to actively rescue optimizers from slowdowns encountered in neural network training. The method, bypass pipeline, revitalizes the optimizer by extending the model space and later contracts the model back to its original space with function-preserving algebraic constraints. We implement the method into the bypass algorithm, verify that the algorithm shows theoretically expected behaviors of bypassing, and demonstrate its empirical benefit in regression and classification benchmarks. Bypass algorithm is highly practical, as it is computationally efficient and compatible with other improvements of first-order optimizers. In addition, bypassing for neural networks leads to new theoretical research such as model-specific bypassing and neural architecture search (NAS).'

doi: 10.1109/TNNLS.2024.3411020
links:
- name: URL
  url: https://ieeexplore.ieee.org/document/10569069
# url_pdf: 'lee-powell-BCQ-2019-TAC.pdf'

tags:
- Deep learning
- Theory

image:
  caption: 'Bypassing-in-a-nutshell diagram'
  focal_point: 'center'
  preview_only: false

featured: true 

---
