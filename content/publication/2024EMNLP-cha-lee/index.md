---
title: 'Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts'
authors:
- Taehun Cha
- admin

date: '2024-11-15'
publishDate: '2024-11-15T00:00:00.000001Z'
publication_types:
- paper-conference
publication: '*Findings of the Association for Computational Linguistics: EMNLP 2024*'
publication_short: 'In *EMNLP 2024 Findings*'

abstract: In this work, we show the pre-trained language models return distinguishable generation probability and uncertainty distribution to unfaithfully hallucinated texts, regardless of their size and structure. By examining 24 models on 6 data sets, we find out that 88-98% of cases return statistically significantly distinguishable generation probability and uncertainty distributions. Using this general phenomenon, we showcase a hallucination-reducing training algorithm. Our algorithm outperforms other baselines by achieving higher faithfulness metrics while maintaining sound general text quality measures.

links:
- name: EMNLP
  url: 'https://aclanthology.org/2024.findings-emnlp.738/'

url_pdf: 'papers/EMNLP2024-cha-lee.pdf'

tags: ['Hallucination','NLP','LLM']

---
